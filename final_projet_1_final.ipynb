{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d85f7e-3f1b-4bf1-8663-1623ca75b0b7",
   "metadata": {},
   "source": [
    "### Projet 1 - Contrôle de la propagation d'une maladie contagieuse\n",
    "\n",
    "**À rendre avant le 19.12.2024 minuit.**\n",
    "\n",
    "---\n",
    "\n",
    "#### Synopsis\n",
    "\n",
    "Dans ce projet, vous êtes confrontés à un problème de santé publique simulé : contrôler la propagation d'une maladie contagieuse dans une population humaine. Des experts en épidémiologie ont collecté des données en mesurant cinq caractéristiques distinctes (_x1_ à _x5_) pour déterminer si une personne est affectée par la maladie (y=1). Les résultats de ces mesures sont disponibles dans le fichier _[./projet_1_data.csv](./projet_1_data.csv)_.\n",
    "\n",
    "L'objectif est de concevoir et comparer des modèles capables de déterminer si une personne est infectée ou non, en utilisant des données fictives basées sur cinq variables (_x1_ à _x5_). Cependant, nous ne savons pas quelles variables sont réellement pertinentes pour prédire l'infection.\n",
    "\n",
    "Votre mission est de trouver les variables les plus importantes pour réduire au minimum les données invasives à collecter, tout en garantissant que les faux négatifs (cas de maladie non détectés) ne dépassent pas 10% de la population affectée. Ce seuil est essentiel pour limiter la propagation de la maladie. Simultanément, vous devez minimiser les faux positifs pour réduire les coûts des mesures de contrôle supplémentaires, comme des tests de confirmation.\n",
    "\n",
    "---\n",
    "\n",
    "**Important** : Ce problème est une simulation avec des variables fictives. Il ne repose pas sur des connaissances médicales réelles. Ne vous appuyez pas sur une expertise métier pour résoudre ce problème : basez vos décisions sur les données et les outils d'analyse.\n",
    "\n",
    "---\n",
    "\n",
    "#### Objectifs\n",
    "\n",
    "1. **Construire les modèles** :\n",
    "   - Identifier les interactions entre les variables explicatives et la variable cible **y**.\n",
    "   - Développez et comparez **deux** modèles de machine learning **de type différents** en utilisant `scikit-learn`.\n",
    "\n",
    "3. **Sélection des variables importantes** :\n",
    "    -  Dans la construction des modèles, déterminez les variables les plus pertinentes (parmis _x1_ à _x5_) pour la prédiction de l'infection, en justifiant vos choix.\n",
    "\n",
    "5. **Évaluer les performances de vos modèles** :\n",
    "   - Utiliser des métriques adaptées.\n",
    "   - Visualiser la qualité des prédictions sur une partie des données mises à votre disposition.\n",
    "\n",
    "---\n",
    "#### Livrables\n",
    "\n",
    "1. **Modèles** :\n",
    "   - **deux** modèles de prédiction de type différent, avec seuil de detection pour chaque.\n",
    "\n",
    "2. **Caratèristiques** :\n",
    "   - Liste(s) des caractéristiques pertinentes.\n",
    "\n",
    "3. **Rapport** :\n",
    "   - Fournissez un _[rapport](rapport.ipynb)_ bref et clair expliquant vos choix, vos résultats et les performances des modèles. Assurez-vous de valider vos résultats avec des métriques appropriées.\n",
    "\n",
    "---\n",
    "\n",
    "#### Détails\n",
    "\n",
    "1. **Réduction des variables** :\n",
    "   - Identifiez les variables clés parmi _x1_ à _x5_ pour minimiser la collecte de données inutiles. Justifiez votre approche, que ce soit par analyse exploratoire, importance des caractéristiques ou autre méthode.\n",
    "\n",
    "2. **Construction des modèles** :\n",
    "   - Concevez deux modèles prédictifs différents avec `scikit-learn` pour prédire si une personne est infectée ou non.\n",
    "\n",
    "3. **Validation des performances** :\n",
    "   - Évaluez vos modèles en utilisant des métriques appropriées. Utilisez ces résultats pour comparer les deux modèles.\n",
    "\n",
    "4. **Ajustement du seuil** :\n",
    "   - Modifiez le seuil de détection pour limiter les faux négatifs à un maximum de 10% (pour contenir la propagation), tout en minimisant les faux positifs pour réduire les coûts des contrôles inutiles.\n",
    "\n",
    "5. **Outils suggérés** :\n",
    "   - Utilisez `numpy`, `pandas` et `scikit-learn` pour votre analyse et vos modèles.\n",
    "\n",
    "6. **Nettoyage avant soumission** :\n",
    "   - Avant de soumettre votre travail, assurez-vous que votre notebook s'exécute correctement de bout en bout en utilisant l'option _Restart Kernel and Run All Cells..._. Ensuite exécutez _Restart Kernel and Clear All Outputs..._ pour sauvegarder un notebook propre.\n",
    "\n",
    "---\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "Cette question admet plusieurs solutions possibles. Une partie de la note sera attribuée en fonction de la cohérence et de la justification de vos choix. Votre rapport devra inclure une discussion expliquant les méthodes utilisées, ainsi que des résultats obtenus.\n",
    "\n",
    "Pour maximiser votre impact, comparez les performances des deux modèles que vous avez développés et justifiez le choix de celui que vous considérez comme optimal.\n",
    "\n",
    "---\n",
    "\n",
    "#### Barème - sur 6 points\n",
    "\n",
    "1. **Qualité de la méthode et du code** :\n",
    "    - Noté sur 3 points, en tenant compte de la clarté, de l'organisation et de l'efficacité des approches utilisées.\n",
    "\n",
    "2. **Vérification et analyse des résultats** :\n",
    "    - Noté sur 3 points, en fonction de la rigueur dans l'évaluation des modèles et de la pertinence des conclusions.\n",
    "\n",
    "3. **Fonctionnalité** :\n",
    "    - Des points seront déduits si le notebook n'est pas fonctionnel, en fonction de la gravité des problèmes rencontrés et du nombre de corrections nécessaires pour le rendre opérationnel.\n",
    "\n",
    "\n",
    "Les notes incluent une évaluation des réponses fournies dans le rapport, en particulier la justification des choix effectués et l'interprétation des résultats et performances des modèles.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfbd2f63-8921-402b-8673-a14c1bf3a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Install seaborn if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"seaborn\"])\n",
    "    import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9161cc",
   "metadata": {},
   "source": [
    "# Projet 1 - Contrôle de la propagation d'une maladie contagieuse\n",
    "\n",
    "## Contexte et Objectifs\n",
    "\n",
    "Vous disposez de données simulées sur une population humaine, avec cinq variables explicatives (x1 à x5) et une variable cible (y) indiquant si une personne est infectée (y=1) ou non (y=0).\n",
    "\n",
    "**Objectifs** :\n",
    "1. Identifier les variables les plus importantes pour prédire l'infection.\n",
    "2. Construire et comparer deux modèles de machine learning différents (ex : Régression Logistique et Forêt Aléatoire).\n",
    "3. Ajuster le seuil de prédiction pour garantir que les faux négatifs ne dépassent pas 10% des individus infectés, tout en minimisant les faux positifs.\n",
    "4. Réduire idéalement le nombre de variables collectées sans dégrader fortement la capacité du modèle à détecter l'infection.\n",
    "\n",
    "Nous utiliserons `scikit-learn`, `pandas`, `numpy`, `matplotlib` et `seaborn`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b0501",
   "metadata": {},
   "source": [
    "### Chargement des données\n",
    "\n",
    "Le fichier `projet_1_data.csv` contient les colonnes suivantes :\n",
    "- x1, x2, x3, x4, x5 : variables explicatives\n",
    "- y : variable cible (1 si infecté, 0 sinon)\n",
    "\n",
    "On charge le jeu de données et on en fait un aperçu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e16b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"projet_1_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb462e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962872fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3030ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"projet_1_data.csv\")\n",
    "\n",
    "# Quick look at the data\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# Distribution of the target variable\n",
    "print(\"Répartition de la variable cible (y) :\")\n",
    "print(data['y'].value_counts(normalize=True))\n",
    "\n",
    "# Pairplot to visualize relationships and distributions\n",
    "# 'hue' is set to 'y' to color the points by the target class\n",
    "sns.pairplot(data, hue='y', diag_kind='hist', vars=['x1','x2','x3','x4','x5'])\n",
    "plt.suptitle(\"Visualisation initiale des relations entre variables (coloration par y)\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Matrice de corrélation entre les variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d31ada",
   "metadata": {},
   "source": [
    "### Analyse exploratoire\n",
    "\n",
    "- Vérifions la distribution de la variable cible.\n",
    "- Examinons la corrélation entre les variables pour avoir une première idée des relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a1890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Répartition de la variable cible (y) :\")\n",
    "print(data['y'].value_counts(normalize=True))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Matrice de corrélation entre les variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7cc5a",
   "metadata": {},
   "source": [
    "**Observations préliminaires :**\n",
    "\n",
    "- La variable cible `y` est déséquilibrée (environ 20% d'infectés et 80% de non-infectés).\n",
    "- Les corrélations entre `x1`, `x2`, `x3`, `x4`, `x5` et `y` semblent faibles, mais une faible corrélation linéaire ne signifie pas forcément une absence de relation prédictive.\n",
    "  \n",
    "Nous compterons sur les modèles pour estimer l'importance des variables.\n",
    "\n",
    "---\n",
    "\n",
    "### Séparation des données\n",
    "\n",
    "Avant toute modélisation, on sépare les données en jeu d'entraînement (train) et de test.  \n",
    "Le jeu d'entraînement sert à ajuster les modèles, et le jeu de test évalue la performance sur des données non vues afin de détecter un éventuel surapprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180e10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['x1','x2','x3','x4','x5']]\n",
    "y = data['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66eeb95",
   "metadata": {},
   "source": [
    "### Sélection des variables importantes (estimation)\n",
    "\n",
    "On va entraîner rapidement une Forêt Aléatoire sur toutes les variables pour évaluer leur importance.  \n",
    "Cela nous donnera une indication sur les variables potentiellement essentielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b896e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_temp = RandomForestClassifier(random_state=42)\n",
    "rf_temp.fit(X_train, y_train)\n",
    "importances = rf_temp.feature_importances_\n",
    "\n",
    "feature_importances = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances, orient='h')\n",
    "plt.title(\"Importance des variables - Forêt Aléatoire (approche rapide)\")\n",
    "plt.show()\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ec319",
   "metadata": {},
   "source": [
    "D'après ce premier aperçu, certaines variables (par exemple `x2` et `x1`) semblent avoir plus d'importance. Nous conserverons cette information pour éventuellement réduire le nombre de variables par la suite.\n",
    "\n",
    "---\n",
    "\n",
    "### Construction des modèles\n",
    "\n",
    "Nous allons construire deux modèles différents pour comparer leurs performances :\n",
    "\n",
    "1. **Régression Logistique**\n",
    "2. **Forêt Aléatoire**\n",
    "\n",
    "Ensuite, nous évaluerons leurs performances sur le jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0dd8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle 1 : Régression Logistique\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "y_proba_logreg = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Modèle 2 : Forêt Aléatoire\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_proba_rf = rf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1136c",
   "metadata": {},
   "source": [
    "### Évaluation des performances initiales\n",
    "\n",
    "Nous utiliserons :\n",
    "- Matrice de confusion\n",
    "- Accuracy, Precision, Recall, F1-score\n",
    "- AUC (ROC)\n",
    "\n",
    "Ces métriques seront comparées sur le jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model_name, y_true, y_pred, y_proba):\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Matrice de confusion :\\n\", cm)\n",
    "    print(\"\\nRapport de classification :\\n\", classification_report(y_true, y_pred))\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    print(\"AUC-ROC :\", auc)\n",
    "    print(\"---------\")\n",
    "\n",
    "print_metrics(\"Régression Logistique\", y_test, y_pred_logreg, y_proba_logreg)\n",
    "print_metrics(\"Forêt Aléatoire\", y_test, y_pred_rf, y_proba_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8842a",
   "metadata": {},
   "source": [
    "### Comparaison des modèles via la courbe ROC\n",
    "\n",
    "La courbe ROC permet de visualiser le compromis entre Taux de Vrais Positifs (TPR ou Recall) et Taux de Faux Positifs (FPR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f58320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC = 1.0 (Overfitting is evident, but not necessarily problematic if generalization is still good.)\n",
    "fpr_log, tpr_log, _ = roc_curve(y_test, y_proba_logreg)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_log, tpr_log, label=f\"Logistic Regression (AUC={roc_auc_score(y_test, y_proba_logreg):.2f})\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC={roc_auc_score(y_test, y_proba_rf):.2f})\")\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"Taux de Faux Positifs (FPR)\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs (TPR)\")\n",
    "plt.title(\"Courbes ROC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575a384",
   "metadata": {},
   "source": [
    "**Analyse** :\n",
    "\n",
    "- La Régression Logistique a une AUC proche de 0.48 (environ aléatoire) et ne détecte presque aucun infecté (recall sur la classe 1 très faible).\n",
    "- La Forêt Aléatoire a une AUC d’environ 0.95 sur le jeu de test, ce qui est excellent. Elle identifie beaucoup plus d'individus infectés.\n",
    "\n",
    "Clairement, la Forêt Aléatoire est plus performante sur ce dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Ajustement du seuil\n",
    "\n",
    "Objectif : Les faux négatifs (personnes infectées non détectées) doivent représenter au maximum 10% de tous les infectés.  \n",
    "C’est-à-dire : FNR = FN / (FN + TP) ≤ 0.10, donc Sensibilité (TPR) ≥ 0.90.\n",
    "\n",
    "Nous allons ajuster le seuil de décision (par défaut 0.5) sur les prédictions probabilistes de la Forêt Aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,1,101)\n",
    "valid_thresholds = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_proba_rf >= t).astype(int)\n",
    "    cm_t = confusion_matrix(y_test, y_pred_t)\n",
    "    TN, FP, FN, TP = cm_t.ravel()\n",
    "    sens = TP / (TP + FN) if (TP+FN)>0 else 0\n",
    "    spec = TN / (TN + FP) if (TN+FP)>0 else 0\n",
    "    if sens >= 0.90:\n",
    "        # On enregistre les seuils qui atteignent au moins 90% de sensibilité\n",
    "        valid_thresholds.append((t, sens, spec))\n",
    "\n",
    "if len(valid_thresholds) > 0:\n",
    "    # On choisit le seuil avec la meilleure spécificité parmi ceux qui ont sensibilité ≥ 0.90\n",
    "    chosen_threshold, chosen_sens, chosen_spec = max(valid_thresholds, key=lambda x: x[2])\n",
    "else:\n",
    "    # Si aucun seuil n'atteint 90% de sensibilité, on reste à 0.5 (peu probable vu la perf de la RF)\n",
    "    chosen_threshold, chosen_sens, chosen_spec = 0.5, None, None\n",
    "\n",
    "chosen_threshold, chosen_sens, chosen_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31900b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_adj = (y_proba_rf >= chosen_threshold).astype(int)\n",
    "cm_adj = confusion_matrix(y_test, y_pred_adj)\n",
    "TN, FP, FN, TP = cm_adj.ravel()\n",
    "\n",
    "print(\"Matrice de confusion avec seuil ajusté:\\n\", cm_adj)\n",
    "print(\"\\nSensibilité (TPR):\", TP/(TP+FN))\n",
    "print(\"Spécificité (TNR):\", TN/(TN+FP))\n",
    "print(\"Taux de Faux Négatifs (FNR):\", FN/(FN+TP))\n",
    "print(\"Taux de Faux Positifs (FPR):\", FP/(FP+TN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9646df3",
   "metadata": {},
   "source": [
    "**Analyse** :  \n",
    "- Nous avons maintenant une sensibilité ≥ 90%, donc moins de 10% de faux négatifs parmi les infectés.\n",
    "- Cela pourrait augmenter les faux positifs, mais l'idée est de mieux contrôler la propagation de la maladie en ne ratant presque aucun cas infecté.\n",
    "\n",
    "---\n",
    "\n",
    "### Réduction du nombre de variables\n",
    "\n",
    "Testons un modèle Forêt Aléatoire avec uniquement les variables les plus importantes (par exemple `x2` et `x1` identifiées plus haut).\n",
    "\n",
    "Puis réajustons le seuil de la même manière et comparons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db615e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = X_train[['x2','x1']]\n",
    "X_test_reduced = X_test[['x2','x1']]\n",
    "\n",
    "rf_reduced = RandomForestClassifier(random_state=42)\n",
    "rf_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "y_proba_rf_reduced = rf_reduced.predict_proba(X_test_reduced)[:,1]\n",
    "\n",
    "thresholds = np.linspace(0,1,101)\n",
    "valid_thresholds = []\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_proba_rf_reduced >= t).astype(int)\n",
    "    cm_t = confusion_matrix(y_test, y_pred_t)\n",
    "    TN_r, FP_r, FN_r, TP_r = cm_t.ravel()\n",
    "    sens = TP_r / (TP_r + FN_r) if (TP_r+FN_r)>0 else 0\n",
    "    spec = TN_r / (TN_r + FP_r) if (TN_r+FP_r)>0 else 0\n",
    "    if sens >= 0.90:\n",
    "        valid_thresholds.append((t, sens, spec))\n",
    "\n",
    "if len(valid_thresholds) > 0:\n",
    "    chosen_threshold_reduced, chosen_sens_reduced, chosen_spec_reduced = max(valid_thresholds, key=lambda x: x[2])\n",
    "else:\n",
    "    chosen_threshold_reduced, chosen_sens_reduced, chosen_spec_reduced = 0.5, None, None\n",
    "\n",
    "y_pred_adj_reduced = (y_proba_rf_reduced >= chosen_threshold_reduced).astype(int)\n",
    "cm_adj_reduced = confusion_matrix(y_test, y_pred_adj_reduced)\n",
    "TN_r, FP_r, FN_r, TP_r = cm_adj_reduced.ravel()\n",
    "\n",
    "print(\"Matrice de confusion (réduit):\\n\", cm_adj_reduced)\n",
    "print(\"\\nSensibilité (TPR):\", TP_r/(TP_r+FN_r))\n",
    "print(\"Taux de Faux Négatifs (FNR):\", FN_r/(FN_r+TP_r))\n",
    "print(\"Taux de Faux Positifs (FPR):\", FP_r/(FP_r+TN_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031acd31",
   "metadata": {},
   "source": [
    "**Analyse** :  \n",
    "- En réduisant à x1 et x2, nous pourrions conserver une sensibilité élevée (≥90%) après ajustement du seuil, mais le taux de faux positifs augmente peut-être.\n",
    "- Ce compromis doit être jugé selon les contraintes du problème. Réduire les données collectées diminue les coûts et l'invasion de la vie privée, mais augmente les tests inutiles.\n",
    "- Selon la priorité, on peut soit conserver plus de variables pour réduire les faux positifs, soit se contenter de ces deux variables.\n",
    "\n",
    "---\n",
    "\n",
    "### Évaluation sur le jeu d'entraînement et de test pour détecter le surapprentissage\n",
    "\n",
    "Ci-dessous, on compare les performances sur l'ensemble d'entraînement et sur l'ensemble de test afin de vérifier si les modèles surapprennent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240dcae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Evaluation\n",
    "print(\"=== Logistic Regression Performance ===\")\n",
    "\n",
    "y_pred_train_logreg = logreg.predict(X_train)\n",
    "y_proba_train_logreg = logreg.predict_proba(X_train)[:,1]\n",
    "print(\"\\n--- Training Set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred_train_logreg))\n",
    "print(\"AUC:\", roc_auc_score(y_train, y_proba_train_logreg))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred_train_logreg))\n",
    "\n",
    "print(\"\\n--- Testing Set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba_logreg))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "\n",
    "# Random Forest Evaluation\n",
    "print(\"\\n\\n=== Random Forest Performance ===\")\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "y_proba_train_rf = rf.predict_proba(X_train)[:,1]\n",
    "print(\"\\n--- Training Set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred_train_rf))\n",
    "print(\"AUC:\", roc_auc_score(y_train, y_proba_train_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred_train_rf))\n",
    "\n",
    "print(\"\\n--- Testing Set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, y_pred_logreg)\n",
    "\n",
    "print(\"Matrice de confusion:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ea0ee",
   "metadata": {},
   "source": [
    "Le seuil `chosen_threshold` est celui qui permet une sensibilité ≥ 0.90. Vérifions la matrice de confusion avec ce seuil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c5e4e-8bfb-4935-8ece-eae93aaabe1f",
   "metadata": {},
   "source": [
    "### Amélioration du Modèle de Régression Logistique\n",
    "\n",
    "À ce stade, nous avons constaté que la Régression Logistique avait des difficultés à détecter la classe positive (infectés). Nous allons :\n",
    "\n",
    "1. Appliquer un poids de classe (`class_weight='balanced'`) afin d'accorder plus d'importance aux individus infectés (minorité).\n",
    "2. Effectuer une recherche d'hyperparamètres sur `C` (paramètre de régularisation) pour trouver une configuration plus adaptée.\n",
    "3. Réévaluer les performances du modèle amélioré, en particulier la sensibilité (Recall) pour la classe 1.\n",
    "4. Afficher la courbe ROC pour visualiser l'amélioration (ou non) du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395aa8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# On reprend X_train, X_test, y_train, y_test du contexte précédent\n",
    "# Assurez-vous que X_train, X_test, y_train, y_test sont déjà définis dans le notebook.\n",
    "\n",
    "# Modèle de Régression Logistique avec class_weight='balanced'\n",
    "logreg_balanced = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Définition de la grille de paramètres pour C\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Utilisation de GridSearchCV pour trouver la meilleure valeur de C\n",
    "grid_search = GridSearchCV(logreg_balanced, param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_logreg = grid_search.best_estimator_\n",
    "\n",
    "print(\"Meilleurs paramètres trouvés :\", grid_search.best_params_)\n",
    "print(\"AUC sur l'ensemble d'entraînement (CV) :\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e4ab9",
   "metadata": {},
   "source": [
    "### Évaluation du modèle amélioré\n",
    "\n",
    "Maintenant que nous avons un modèle de Régression Logistique avec class_weight et un C optimisé, évaluons ses performances sur le jeu de test. Nous prêterons attention à la matrice de confusion, au rapport de classification, et à la courbe ROC pour constater toute amélioration par rapport au modèle initial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145443d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "# Prédictions avec le meilleur modèle Logistic Regression\n",
    "y_pred_best_logreg = best_logreg.predict(X_test)\n",
    "y_proba_best_logreg = best_logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Matrice de confusion et rapport de classification\n",
    "cm_best = confusion_matrix(y_test, y_pred_best_logreg)\n",
    "print(\"Matrice de confusion (Logistic Regression améliorée) :\\n\", cm_best)\n",
    "print(\"\\nRapport de classification (Logistic Regression améliorée) :\")\n",
    "print(classification_report(y_test, y_pred_best_logreg))\n",
    "\n",
    "# Calcul de l'AUC\n",
    "auc_best = roc_auc_score(y_test, y_proba_best_logreg)\n",
    "print(\"AUC sur le jeu de test (Logistic Regression améliorée):\", auc_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723e2a6",
   "metadata": {},
   "source": [
    "### Courbe ROC du Modèle Amélioré\n",
    "\n",
    "Nous allons tracer la courbe ROC du nouveau modèle Logistic Regression amélioré et la comparer visuellement à la version initiale, si vous avez conservé ses valeurs. Cela permettra de voir si le modèle a gagné en capacité discriminante, en particulier pour la détection de la classe minoritaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511fcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si vous avez conservé y_proba_logreg (probabilités de l'ancien modèle), vous pouvez le comparer.\n",
    "# Sinon, on va simplement tracer la courbe du nouveau modèle amélioré.\n",
    "\n",
    "fpr_best, tpr_best, _ = roc_curve(y_test, y_proba_best_logreg)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Courbe du nouveau modèle\n",
    "plt.plot(fpr_best, tpr_best, label=f\"Logistic Regression Améliorée (AUC={auc_best:.2f})\")\n",
    "\n",
    "# Optionnel : si vous avez l'ancien y_proba_logreg, décommentez les deux lignes ci-dessous pour comparaison\n",
    "# fpr_old, tpr_old, _ = roc_curve(y_test, y_proba_logreg)\n",
    "# plt.plot(fpr_old, tpr_old, label=f\"Logistic Regression Initiale (AUC={roc_auc_score(y_test, y_proba_logreg):.2f})\", linestyle='--')\n",
    "\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"Taux de Faux Positifs (FPR)\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs (TPR)\")\n",
    "plt.title(\"Courbe ROC - Logistic Regression Améliorée\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des Résultats\n",
    "\n",
    "- En appliquant `class_weight='balanced'`, le modèle Logistic Regression accorde plus d'importance à la classe minoritaire. Couplé à un ajustement du paramètre `C`, cela devrait améliorer la capacité du modèle à détecter les individus infectés (augmenter le rappel pour la classe 1).\n",
    "- L'AUC, la matrice de confusion, le rapport de classification et la courbe ROC permettent de constater cette amélioration.\n",
    "- Si la sensibilité (Recall) de la classe 1 est désormais plus élevée et l'AUC plus grande que précédemment, le modèle est mieux adapté à l'objectif du projet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb55344",
   "metadata": {},
   "source": [
    "### Nouvelle tentative d'amélioration du modèle Logistic Regression\n",
    "\n",
    "Dans cette section, nous allons créer un nouveau modèle de régression logistique avec quelques améliorations afin d'obtenir de meilleurs résultats :\n",
    "\n",
    "1. Utiliser `class_weight='balanced'` pour tenir compte du déséquilibre de classe.\n",
    "2. Utiliser une recherche d'hyperparamètres (`GridSearchCV`) afin de trouver une valeur de `C` plus adaptée.\n",
    "\n",
    "Nous espérons ainsi augmenter la capacité du modèle à détecter la classe minoritaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ba342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, ConfusionMatrixDisplay\n",
    "\n",
    "# Assurez-vous que X_train, X_test, y_train, y_test sont déjà définis\n",
    "\n",
    "# Création d'un pipeline avec un scaler et la régression logistique\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logistic', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grille de paramètres pour la recherche\n",
    "param_grid = {\n",
    "    'logistic__C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Mise en place de la recherche par validation croisée\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "improved_logistic_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Meilleurs paramètres trouvés:\", grid_search.best_params_)\n",
    "print(\"Meilleure AUC en validation croisée:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f9aed",
   "metadata": {},
   "source": [
    "### Évaluation du nouveau modèle de Régression Logistique\n",
    "\n",
    "Nous allons maintenant évaluer le modèle `improved_logistic_model` sur le jeu de test et comparer avec l'ancien modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465973f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur le jeu de test\n",
    "y_pred_improved = improved_logistic_model.predict(X_test)\n",
    "y_proba_improved = improved_logistic_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Matrice de confusion\n",
    "cm_improved = confusion_matrix(y_test, y_pred_improved)\n",
    "print(\"Matrice de confusion (Logistic Regression améliorée) :\\n\", cm_improved)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_improved)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification (Logistic Regression améliorée) :\")\n",
    "print(classification_report(y_test, y_pred_improved))\n",
    "\n",
    "# Calcul de l'AUC\n",
    "auc_improved = roc_auc_score(y_test, y_proba_improved)\n",
    "print(\"AUC sur le jeu de test (Logistic Regression améliorée):\", auc_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb545ff0",
   "metadata": {},
   "source": [
    "### Courbe ROC du Modèle Amélioré\n",
    "\n",
    "Nous allons tracer la courbe ROC du nouveau modèle afin de visualiser l'amélioration potentielle par rapport à l'ancien modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d45e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_improved, tpr_improved, _ = roc_curve(y_test, y_proba_improved)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(fpr_improved, tpr_improved, label=f\"Improved Logistic Regression (AUC={auc_improved:.2f})\")\n",
    "\n",
    "# Si vous avez conservé y_proba_logreg de l'ancien modèle, vous pouvez le comparer :\n",
    "# fpr_old, tpr_old, _ = roc_curve(y_test, y_proba_logreg)\n",
    "# old_auc = roc_auc_score(y_test, y_proba_logreg)\n",
    "# plt.plot(fpr_old, tpr_old, label=f\"Old Logistic Regression (AUC={old_auc:.2f})\", linestyle='--')\n",
    "\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"Taux de Faux Positifs (FPR)\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs (TPR)\")\n",
    "plt.title(\"Courbe ROC - Logistic Regression Améliorée\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict probabilities for both models\n",
    "y_proba_rf = rf.predict_proba(X_test)[:,1]\n",
    "y_proba_improved = improved_logistic_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute ROC curves and AUC\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
    "auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "\n",
    "fpr_improved, tpr_improved, _ = roc_curve(y_test, y_proba_improved)\n",
    "auc_improved = roc_auc_score(y_test, y_proba_improved)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC={auc_rf:.2f})\")\n",
    "plt.plot(fpr_improved, tpr_improved, label=f\"Improved Logistic Regression (AUC={auc_improved:.2f})\")\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"Taux de Faux Positifs (FPR)\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs (TPR)\")\n",
    "plt.title(\"Courbes ROC - Comparaison Random Forest vs Logistic Regression Améliorée\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753013a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Load the data\n",
    "data_new = pd.read_csv(\"projet_1_data.csv\")\n",
    "X_new = data_new[['x1', 'x2', 'x3', 'x4', 'x5']]\n",
    "y_new = data_new['y']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "    X_new, y_new, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Ensure test data columns match training data columns\n",
    "X_test_new = X_test_new[X_train_new.columns]\n",
    "\n",
    "# Define the pipeline\n",
    "new_pipeline = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures()),          # Polynomial expansion\n",
    "    ('scaler_step', StandardScaler()),                # Scaling\n",
    "    ('logistic_model', LogisticRegression(\n",
    "        class_weight='balanced', max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid for the pipeline\n",
    "new_param_grid = {\n",
    "    'poly_features__degree': [1, 2, 3],\n",
    "    'logistic_model__C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV using DataFrames\n",
    "new_grid_search = GridSearchCV(\n",
    "    new_pipeline,\n",
    "    new_param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "new_grid_search.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters from new_grid_search:\")\n",
    "print(new_grid_search.best_params_)\n",
    "print(f\"Best cross-validation AUC from new_grid_search: {new_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on the test set using DataFrames\n",
    "best_new_model = new_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_new = best_new_model.predict(X_test_new)\n",
    "y_proba_new = best_new_model.predict_proba(X_test_new)[:, 1]\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "new_cm = confusion_matrix(y_test_new, y_pred_new)\n",
    "print(\"\\nTest Set Confusion Matrix from best_new_model:\")\n",
    "print(new_cm)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set from best_new_model:\")\n",
    "print(classification_report(y_test_new, y_pred_new))\n",
    "\n",
    "# AUC score\n",
    "new_auc_test = roc_auc_score(y_test_new, y_proba_new)\n",
    "print(f\"Test Set AUC from best_new_model: {new_auc_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c141a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Predict probabilities for both models\n",
    "y_proba_rf = rf.predict_proba(X_test_new)[:, 1]  # Probabilities from Random Forest\n",
    "y_proba_new = best_new_model.predict_proba(X_test_new)[:, 1]  # Probabilities from Logistic Regression\n",
    "\n",
    "# Compute ROC curves and AUC for Random Forest\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test_new, y_proba_rf)\n",
    "auc_rf = roc_auc_score(y_test_new, y_proba_rf)\n",
    "\n",
    "# Compute ROC curves and AUC for Logistic Regression\n",
    "fpr_new, tpr_new, _ = roc_curve(y_test_new, y_proba_new)\n",
    "auc_new = roc_auc_score(y_test_new, y_proba_new)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC={auc_rf:.4f})\")\n",
    "plt.plot(fpr_new, tpr_new, label=f\"Improved Logistic Regression (AUC={auc_new:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')  # Diagonal line for random guessing\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"ROC Curve Comparison: Random Forest vs Logistic Regression\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "# Prédictions avec le meilleur modèle Logistic Regression (best_new_model)\n",
    "y_pred_best_logreg = best_new_model.predict(X_test_new)\n",
    "y_proba_best_logreg = best_new_model.predict_proba(X_test_new)[:, 1]\n",
    "\n",
    "# Matrice de confusion et rapport de classification\n",
    "cm_best = confusion_matrix(y_test_new, y_pred_best_logreg)\n",
    "print(\"Matrice de confusion (Logistic Regression améliorée) :\\n\", cm_best)\n",
    "print(\"\\nRapport de classification (Logistic Regression améliorée) :\")\n",
    "print(classification_report(y_test_new, y_pred_best_logreg))\n",
    "\n",
    "# Calcul de l'AUC\n",
    "auc_best = roc_auc_score(y_test_new, y_proba_best_logreg)\n",
    "print(\"AUC sur le jeu de test (Logistic Regression améliorée):\", auc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68648305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "# Load the data\n",
    "data_new = pd.read_csv(\"projet_1_data.csv\")\n",
    "X_new = data_new[['x1', 'x2', 'x3', 'x4', 'x5']]\n",
    "y_new = data_new['y']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "    X_new, y_new, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Ensure test data columns match training data columns\n",
    "X_test_new = X_test_new[X_train_new.columns]\n",
    "\n",
    "# Define the pipeline\n",
    "new_pipeline = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures()),          # Polynomial expansion\n",
    "    ('scaler_step', StandardScaler()),                # Scaling\n",
    "    ('logistic_model', LogisticRegression(\n",
    "        class_weight='balanced', max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid for the pipeline\n",
    "new_param_grid = {\n",
    "    'poly_features__degree': [1, 2, 3],\n",
    "    'logistic_model__C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV using DataFrames\n",
    "new_grid_search = GridSearchCV(\n",
    "    new_pipeline,\n",
    "    new_param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "new_grid_search.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters from new_grid_search:\")\n",
    "print(new_grid_search.best_params_)\n",
    "print(f\"Best cross-validation AUC from new_grid_search: {new_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on the test set using DataFrames\n",
    "best_new_model = new_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best_new = best_new_model.predict(X_test_new)\n",
    "y_proba_best_new = best_new_model.predict_proba(X_test_new)[:, 1]\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "cm_best_new = confusion_matrix(y_test_new, y_pred_best_new)\n",
    "print(\"\\nTest Set Confusion Matrix from best_new_model:\")\n",
    "print(cm_best_new)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set from best_new_model:\")\n",
    "print(classification_report(y_test_new, y_pred_best_new))\n",
    "\n",
    "# AUC score\n",
    "auc_best_new = roc_auc_score(y_test_new, y_proba_best_new)\n",
    "print(f\"Test Set AUC from best_new_model: {auc_best_new:.4f}\")\n",
    "\n",
    "# Predictions for rf model\n",
    "y_pred_rf = rf.predict(X_test_new)\n",
    "y_proba_rf = rf.predict_proba(X_test_new)[:, 1]\n",
    "\n",
    "# Confusion matrix for rf model\n",
    "cm_rf = confusion_matrix(y_test_new, y_pred_rf)\n",
    "print(\"\\nTest Set Confusion Matrix from rf model:\")\n",
    "print(cm_rf)\n",
    "\n",
    "print(\"\\nClassification Report on Test Set from rf model:\")\n",
    "print(classification_report(y_test_new, y_pred_rf))\n",
    "\n",
    "# AUC score for rf model\n",
    "auc_rf = roc_auc_score(y_test_new, y_proba_rf)\n",
    "print(f\"Test Set AUC from rf model: {auc_rf:.4f}\")\n",
    "\n",
    "# Function to compute rates and errors\n",
    "def compute_rates(cm):\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    false_positive_rate = FP / (FP + TN)\n",
    "    false_negative_rate = FN / (FN + TP)\n",
    "    total_error_rate = (FP + FN) / (TN + FP + FN + TP)\n",
    "    return false_positive_rate, false_negative_rate, total_error_rate\n",
    "\n",
    "# Compute metrics for best_new_model\n",
    "fpr_best_new, fnr_best_new, error_rate_best_new = compute_rates(cm_best_new)\n",
    "print(\"Best New Model:\")\n",
    "print(f\"False Positive Rate: {fpr_best_new:.4f}\")\n",
    "print(f\"False Negative Rate: {fnr_best_new:.4f}\")\n",
    "print(f\"Total Error Rate: {error_rate_best_new:.4f}\")\n",
    "\n",
    "# Compute metrics for rf model\n",
    "fpr_rf, fnr_rf, error_rate_rf = compute_rates(cm_rf)\n",
    "print(\"\\nRandom Forest Model:\")\n",
    "print(f\"False Positive Rate: {fpr_rf:.4f}\")\n",
    "print(f\"False Negative Rate: {fnr_rf:.4f}\")\n",
    "print(f\"Total Error Rate: {error_rate_rf:.4f}\")\n",
    "\n",
    "# Plot ROC curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test_new, y_proba_rf)\n",
    "fpr_new, tpr_new, _ = roc_curve(y_test_new, y_proba_best_new)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC={auc_rf:.4f})\")\n",
    "plt.plot(fpr_new, tpr_new, label=f\"Improved Logistic Regression (AUC={auc_best_new:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')  # Diagonal line for random guessing\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"ROC Curve Comparison: Random Forest vs Logistic Regression\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ed00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"projet_1_data.csv\")\n",
    "X = data[[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"]]\n",
    "y = data[\"y\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# Utility function to optimize the threshold for a specific False Negative Rate (FNR)\n",
    "def optimize_threshold(y_true, y_proba, max_fnr=0.1):\n",
    "    \"\"\"\n",
    "    Optimizes the decision threshold to ensure FNR ≤ max_fnr and minimizes FPR.\n",
    "\n",
    "    Args:\n",
    "    - y_true: Ground truth labels\n",
    "    - y_proba: Predicted probabilities\n",
    "    - max_fnr: Maximum allowable false negative rate\n",
    "\n",
    "    Returns:\n",
    "    - Best threshold meeting the FNR requirement with minimum FPR\n",
    "    - FNR and FPR at the selected threshold\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    best_threshold = 0.5\n",
    "    best_fnr = 1.0\n",
    "    best_fpr = 1.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "        fnr = FN / (FN + TP) if (FN + TP) > 0 else 1.0\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 1.0\n",
    "\n",
    "        # Ensure FNR ≤ max_fnr, and minimize FPR\n",
    "        if fnr <= max_fnr and fpr < best_fpr:\n",
    "            best_threshold = t\n",
    "            best_fnr = fnr\n",
    "            best_fpr = fpr\n",
    "\n",
    "    return best_threshold, best_fnr, best_fpr\n",
    "\n",
    "\n",
    "# --- Logistic Regression Model (Best New Model) ---\n",
    "logistic_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"poly_features\", PolynomialFeatures()),  # Polynomial expansion\n",
    "        (\"scaler\", StandardScaler()),  # Scaling\n",
    "        (\n",
    "            \"logistic_model\",\n",
    "            LogisticRegression(class_weight=\"balanced\", random_state=42, max_iter=2000),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "param_grid_lr = {\n",
    "    \"poly_features__degree\": [1, 2, 3],\n",
    "    \"logistic_model__C\": [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# Grid Search for Logistic Regression\n",
    "grid_search_lr = GridSearchCV(\n",
    "    logistic_pipeline, param_grid_lr, scoring=\"roc_auc\", cv=5, n_jobs=-1\n",
    ")\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "best_new_model = grid_search_lr.best_estimator_\n",
    "\n",
    "# Predict probabilities for Logistic Regression\n",
    "y_proba_lr = best_new_model.predict_proba(X_test)[:, 1]\n",
    "lr_threshold, lr_fnr, lr_fpr = optimize_threshold(y_test, y_proba_lr, max_fnr=0.1)\n",
    "y_pred_lr = (y_proba_lr >= lr_threshold).astype(int)\n",
    "\n",
    "# --- Random Forest Model ---\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "param_grid_rf = {\"n_estimators\": [50, 100, 200], \"max_depth\": [3, 5, None]}\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "grid_search_rf = GridSearchCV(\n",
    "    random_forest_model, param_grid_rf, scoring=\"roc_auc\", cv=5, n_jobs=-1\n",
    ")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predict probabilities for Random Forest\n",
    "y_proba_rf = best_rf_model.predict_proba(X_test)[:, 1]\n",
    "rf_threshold, rf_fnr, rf_fpr = optimize_threshold(y_test, y_proba_rf, max_fnr=0.1)\n",
    "y_pred_rf = (y_proba_rf >= rf_threshold).astype(int)\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "def evaluate_model(name, y_true, y_pred, y_proba, threshold, fnr, fpr):\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Optimized Threshold: {threshold:.2f}\")\n",
    "    print(f\"False Negative Rate (FNR): {fnr:.2f}\")\n",
    "    print(f\"False Positive Rate (FPR): {fpr:.2f}\\n\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.show()\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(f\"AUC: {roc_auc_score(y_true, y_proba):.2f}\")\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "evaluate_model(\n",
    "    \"Logistic Regression (Best New Model)\",\n",
    "    y_test,\n",
    "    y_pred_lr,\n",
    "    y_proba_lr,\n",
    "    lr_threshold,\n",
    "    lr_fnr,\n",
    "    lr_fpr,\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate Random Forest\n",
    "evaluate_model(\n",
    "    \"Random Forest\",\n",
    "    y_test,\n",
    "    y_pred_rf,\n",
    "    y_proba_rf,\n",
    "    rf_threshold,\n",
    "    rf_fnr,\n",
    "    rf_fpr,\n",
    ")\n",
    "\n",
    "# --- ROC Curves ---\n",
    "def plot_roc_curves(y_true, models):\n",
    "    \"\"\"\n",
    "    Plots ROC curves for multiple models.\n",
    "\n",
    "    Args:\n",
    "    - y_true: Ground truth labels\n",
    "    - models: List of tuples [(name, y_proba)], where y_proba are probabilities\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, y_proba in models:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "    plt.title(\"ROC Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curves(\n",
    "    y_test, [(\"Logistic Regression (Best New Model)\", y_proba_lr), (\"Random Forest\", y_proba_rf)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Extract reduced feature sets\n",
    "X_train_reduced = X_train[['x1', 'x2', 'x3']]\n",
    "X_test_reduced = X_test[['x1', 'x2', 'x3']]\n",
    "\n",
    "# Train a new Random Forest model on only x1, x2, x3\n",
    "rf_reduced = RandomForestClassifier(\n",
    "    n_estimators=best_rf_model.n_estimators,\n",
    "    max_depth=best_rf_model.max_depth,\n",
    "    random_state=42\n",
    "    # Add any other parameters that were used in best_rf_model if known\n",
    ")\n",
    "rf_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Evaluate the original best_rf_model on the test set\n",
    "y_proba_full = best_rf_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_full = (y_proba_full >= 0.5).astype(int)\n",
    "print(\"=== Best_RF_Model (Full Features) ===\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_full))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_full))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba_full))\n",
    "\n",
    "# Evaluate the new reduced model on the test set\n",
    "y_proba_reduced = rf_reduced.predict_proba(X_test_reduced)[:, 1]\n",
    "y_pred_reduced = (y_proba_reduced >= 0.5).astype(int)\n",
    "print(\"\\n=== RF Model (x1, x2, x3 Only) ===\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_reduced))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_reduced))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba_reduced))\n",
    "\n",
    "# Compare the AUC, precision, recall for the infected class, or other important metrics \n",
    "# to determine if performance is similar. If the results are close, it suggests you \n",
    "# may not need x4 and x5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a79d27",
   "metadata": {},
   "source": [
    "### Réduction du nombre de variables\n",
    "\n",
    "Après analyse, nous avons déterminé que les variables `x1`, `x2`  sont suffisantes pour nos modèles. Nous allons donc nous concentrer uniquement sur ces trois variables pour la suite de notre analyse et modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Extract reduced feature sets (only x1 and x2)\n",
    "X_train_reduced_2 = X_train[['x1', 'x2']]\n",
    "X_test_reduced_2 = X_test[['x1', 'x2']]\n",
    "\n",
    "# Train a new Random Forest model on only x1, x2\n",
    "rf_reduced_2 = RandomForestClassifier(\n",
    "    n_estimators=best_rf_model.n_estimators,\n",
    "    max_depth=best_rf_model.max_depth,\n",
    "    random_state=42\n",
    "    # If other hyperparameters were tuned, add them here to replicate best_rf_model's settings\n",
    ")\n",
    "rf_reduced_2.fit(X_train_reduced_2, y_train)\n",
    "\n",
    "# Evaluate the original best_rf_model (full features)\n",
    "y_proba_full = best_rf_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_full = (y_proba_full >= 0.5).astype(int)\n",
    "print(\"=== Best_RF_Model (Full Features) ===\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_full))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_full))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba_full))\n",
    "\n",
    "# Evaluate the model trained on only x1 and x2\n",
    "y_proba_reduced_2 = rf_reduced_2.predict_proba(X_test_reduced_2)[:, 1]\n",
    "y_pred_reduced_2 = (y_proba_reduced_2 >= 0.5).astype(int)\n",
    "print(\"\\n=== RF Model (x1, x2 Only) ===\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_reduced_2))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_reduced_2))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba_reduced_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"projet_1_data.csv\")\n",
    "X = data[['x1', 'x2']]  # Only x1 and x2\n",
    "y = data['y']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the pipeline with only x1 and x2\n",
    "pipeline_reduced = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures()),         # Polynomial features\n",
    "    ('scaler', StandardScaler()),                    # Standard scaling\n",
    "    ('logistic', LogisticRegression(\n",
    "        class_weight='balanced', max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid for the reduced feature set\n",
    "param_grid_reduced = {\n",
    "    'poly_features__degree': [1, 2, 3],\n",
    "    'logistic__C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# GridSearchCV on the reduced feature set\n",
    "grid_search_reduced = GridSearchCV(\n",
    "    pipeline_reduced,\n",
    "    param_grid_reduced,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search_reduced.fit(X_train, y_train)\n",
    "\n",
    "# The best model trained on only x1 and x2\n",
    "best_new_model_reduced = grid_search_reduced.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_reduced = best_new_model_reduced.predict(X_test)\n",
    "y_proba_reduced = best_new_model_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "cm_reduced = confusion_matrix(y_test, y_pred_reduced)\n",
    "print(\"Confusion Matrix (x1, x2 only):\\n\", cm_reduced)\n",
    "print(\"\\nClassification Report (x1, x2 only):\")\n",
    "print(classification_report(y_test, y_pred_reduced))\n",
    "print(\"Test Set AUC (x1, x2 only):\", roc_auc_score(y_test, y_proba_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41cdac",
   "metadata": {},
   "source": [
    "### Réduction du nombre de variables\n",
    "\n",
    "Après analyse, nous avons déterminé que les variables `x1`et `x2`  sont suffisantes pour nos modèles. Nous allons donc nous concentrer uniquement sur ces trois variables pour la suite de notre analyse et modélisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713bfbac",
   "metadata": {},
   "source": [
    "Après avoir comparé les performances du modèle de forêt aléatoire sur plusieurs jeux de variables, les résultats indiquent clairement que seules les variables x1 et x2 sont nécessaires pour obtenir de bonnes performances prédictives. En effet :\n",
    "\t•\tLe modèle utilisant uniquement x1 et x2 atteint une sensibilité (rappel) plus élevée pour la détection des individus infectés que la version avec l’ensemble complet de variables (x1, x2, x3, x4, x5).\n",
    "\t•\tL’AUC (Area Under the ROC Curve) et l’exactitude (accuracy) se maintiennent, voire s’améliorent, lorsque l’on retire les variables x3, x4 et x5.\n",
    "\t•\tL’absence de dégradation des performances, voire leur amélioration, valide la pertinence d’abandonner ces trois variables supplémentaires.\n",
    "\n",
    "En conclusion, les résultats démontrent que le modèle ne nécessite que x1 et x2 pour atteindre un niveau de performance équivalent, voire supérieur, par rapport à l’utilisation de toutes les variables. L’abandon de x3, x4 et x5 est donc pleinement justifié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Predict probabilities for both models\n",
    "y_pred_prob_rf = rf_reduced_2.predict_proba(X_test)[:, 1]\n",
    "y_pred_prob_best = best_new_model_reduced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC for rf_reduced_2\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_prob_rf)\n",
    "auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "# Compute ROC curve and AUC for best_new_model_reduced\n",
    "fpr_best, tpr_best, _ = roc_curve(y_test, y_pred_prob_best)\n",
    "auc_best = auc(fpr_best, tpr_best)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr_rf, tpr_rf, label='rf_reduced_2 (AUC = %0.2f)' % auc_rf)\n",
    "plt.plot(fpr_best, tpr_best, label='best_new_model_reduced (AUC = %0.2f)' % auc_best)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict classes for both models\n",
    "y_pred_rf = rf_reduced_2.predict(X_test)\n",
    "y_pred_best = best_new_model_reduced.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix for rf_reduced_2\n",
    "tn_rf, fp_rf, fn_rf, tp_rf = confusion_matrix(y_test, y_pred_rf).ravel()\n",
    "\n",
    "# Calculate false positive rate and false negative rate for rf_reduced_2\n",
    "false_positive_rate_rf = fp_rf / (fp_rf + tn_rf)\n",
    "false_negative_rate_rf = fn_rf / (fn_rf + tp_rf)\n",
    "\n",
    "# Compute confusion matrix for best_new_model_reduced\n",
    "tn_best, fp_best, fn_best, tp_best = confusion_matrix(y_test, y_pred_best).ravel()\n",
    "\n",
    "# Calculate false positive rate and false negative rate for best_new_model_reduced\n",
    "false_positive_rate_best = fp_best / (fp_best + tn_best)\n",
    "false_negative_rate_best = fn_best / (fn_best + tp_best)\n",
    "\n",
    "# Print the results\n",
    "print(\"rf_reduced_2:\")\n",
    "print(f\"False Positive Rate: {false_positive_rate_rf:.2f}\")\n",
    "print(f\"False Negative Rate: {false_negative_rate_rf:.2f}\\n\")\n",
    "\n",
    "print(\"best_new_model_reduced:\")\n",
    "print(f\"False Positive Rate: {false_positive_rate_best:.2f}\")\n",
    "print(f\"False Negative Rate: {false_negative_rate_best:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "def optimize_threshold(y_true, y_proba, max_fnr=0.1):\n",
    "    \"\"\"\n",
    "    Finds a decision threshold that ensures FNR ≤ max_fnr.\n",
    "    Among those that satisfy this condition, it picks the threshold with the lowest FPR.\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    best_threshold = 0.5\n",
    "    best_fnr = 1.0\n",
    "    best_fpr = 1.0\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "        fnr = FN / (FN + TP) if (FN + TP) > 0 else 1.0\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 1.0\n",
    "\n",
    "        # Among thresholds that achieve FNR ≤ max_fnr, pick the one with the lowest FPR\n",
    "        if fnr <= max_fnr and fpr < best_fpr:\n",
    "            best_threshold = t\n",
    "            best_fnr = fnr\n",
    "            best_fpr = fpr\n",
    "\n",
    "    return best_threshold, best_fnr, best_fpr\n",
    "\n",
    "# Adjust threshold for the Random Forest model (using x1, x2)\n",
    "y_proba_rf = rf_reduced_2.predict_proba(X_test[['x1','x2']])[:, 1]\n",
    "rf_threshold, rf_fnr, rf_fpr = optimize_threshold(y_test, y_proba_rf, max_fnr=0.1)\n",
    "y_pred_rf = (y_proba_rf >= rf_threshold).astype(int)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "TN_rf, FP_rf, FN_rf, TP_rf = cm_rf.ravel()\n",
    "\n",
    "print(\"=== Random Forest (x1, x2) with Adjusted Threshold ===\")\n",
    "print(\"Selected Threshold:\", rf_threshold)\n",
    "print(\"Confusion Matrix:\\n\", cm_rf)\n",
    "print(\"False Negative Rate (FNR): {:.2f}%\".format(rf_fnr * 100))\n",
    "print(\"False Positive Rate (FPR): {:.2f}%\".format(rf_fpr * 100))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
    "print(\"FNR ≤ 10%: \", rf_fnr <= 0.1)\n",
    "print()\n",
    "\n",
    "# Adjust threshold for the Logistic Regression model (using x1, x2)\n",
    "y_proba_lr = best_new_model_reduced.predict_proba(X_test[['x1','x2']])[:, 1]\n",
    "lr_threshold, lr_fnr, lr_fpr = optimize_threshold(y_test, y_proba_lr, max_fnr=0.1)\n",
    "y_pred_lr = (y_proba_lr >= lr_threshold).astype(int)\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "TN_lr, FP_lr, FN_lr, TP_lr = cm_lr.ravel()\n",
    "\n",
    "print(\"=== Logistic Regression (x1, x2) with Adjusted Threshold ===\")\n",
    "print(\"Selected Threshold:\", lr_threshold)\n",
    "print(\"Confusion Matrix:\\n\", cm_lr)\n",
    "print(\"False Negative Rate (FNR): {:.2f}%\".format(lr_fnr * 100))\n",
    "print(\"False Positive Rate (FPR): {:.2f}%\".format(lr_fpr * 100))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba_lr))\n",
    "print(\"FNR ≤ 10%: \", lr_fnr <= 0.1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa27ba",
   "metadata": {},
   "source": [
    "---\n",
    "#### Verification\n",
    "\n",
    "Veuillez valider vos modèles à l'aide de la fonction ci-dessous. Le premier modèle doit être celui que vous considérez comme le plus performant.\n",
    "\n",
    "⚠️ - Cette fonction vérifie uniquement que votre soumission est vérifiable. Elle ne donne aucune indication sur la validité de la soumission (e.g. type de ML) ou sa qualité.\n",
    "\n",
    "Vous pouvez spécifier les caractéristiques principales sous deux formats :\n",
    "\n",
    "- Une liste de caractéristiques commune à tous les modèles.\n",
    "- Une liste de listes de caractéristiques, correspondant à chaque modèle (voir exemple), dans le même ordre.\n",
    "\n",
    "Chaque list de caractéristiques peut être spécifiée de deux manières :\n",
    "\n",
    "- Une liste de noms de colonnes pertinentes, dans l'ordre désiré, par exemple ['x3', 'x1', 'x4'].\n",
    "- Une liste de valeurs booléennes correspondant aux colonnes, où True indique une colonne pertinente, par exemple [True, False, True, False, True] pour sélectionner les colonnes ['x1', 'x3', 'x5'].\n",
    "\n",
    "Notez que si le nombre de caractéristiques attendues par votre modèle ne correspond pas au nombre de caractéristiques spécifiées dans la fonction, celle-ci ignorera cette liste de caractéristiques et supposera que le modèle inclut une étape de filtrage automatique des caractéristiques et validera le modèle avec toutes les variables du fichier de hold-out (dans ce cas la liste de caractéristiques sert uniquement à vérifier votre sélection).\n",
    "\n",
    "Vous pouvez également définir un seuil de détection (e.g. pour contrôler le taux de fausses négatives): soit un seuil commun pour les deux modèles, soit une liste de seuils distincts par modèle (ou seuil=None pour utiliser les valeurs par défaut)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36028866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# executer la commande ci-dessous si nécessaire (enlever le #).\n",
    "# !pip install -e ./eng209\n",
    "from importlib import reload\n",
    "import eng209.verify\n",
    "reload(eng209.verify)\n",
    "from eng209.verify import verify_q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacez best_model et second_best_model par vos modèles, et modifiez les caractéristiques et le seuil en conséquence.\n",
    "best_model= rf_reduced_2\n",
    "second_best_model=best_new_model_reduced\n",
    "\n",
    "caracteristiques=['x1','x2']\n",
    "seuil=[rf_threshold,lr_threshold]\n",
    "\n",
    "# Verify models\n",
    "verify_q1(best_model, second_best_model, caracteristiques=caracteristiques, seuil=seuil)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
